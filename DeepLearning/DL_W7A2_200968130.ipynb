{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\n\npd.set_option('display.max_colwidth', 500)\npd.set_option('display.max_rows', 10)\npd.set_option('display.max_columns', 4)","metadata":{"execution":{"iopub.status.busy":"2022-09-26T11:58:03.688163Z","iopub.execute_input":"2022-09-26T11:58:03.688452Z","iopub.status.idle":"2022-09-26T11:58:03.694333Z","shell.execute_reply.started":"2022-09-26T11:58:03.688419Z","shell.execute_reply":"2022-09-26T11:58:03.693603Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Importing Dataset","metadata":{}},{"cell_type":"code","source":"data_train=pd.read_csv(\"../input/imdb-dataset-sentiment-analysis-in-csv-format/Train.csv\")\ndata_valid=pd.read_csv(\"../input/imdb-dataset-sentiment-analysis-in-csv-format/Valid.csv\")\ndata_test=pd.read_csv(\"../input/imdb-dataset-sentiment-analysis-in-csv-format/Test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-09-26T11:58:04.727785Z","iopub.execute_input":"2022-09-26T11:58:04.728451Z","iopub.status.idle":"2022-09-26T11:58:06.095678Z","shell.execute_reply.started":"2022-09-26T11:58:04.728412Z","shell.execute_reply":"2022-09-26T11:58:06.094949Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"def preprocess_data(data = None, stopwords = None, rm_stopwords = True):\n    \"\"\"\n    Data -> pandas DataFrame\n    2 Columns -> 'text','label'\n    \"\"\"\n    def lower_text(data = None):\n        \"\"\"\n        Converts everyword to lower case\n        Returns : Pandas DataFrame\n        \"\"\"\n        data['text'] = data['text'].apply(lambda x : str(x).lower())\n        return data\n        \n    def remove_punctuation(data = None):\n        \"\"\"\n        Removes Punctuation Marks \n        Returns : Pandas DataFrame\n        \"\"\"\n        data['text'] = data['text'].apply(lambda x : re.sub(r'[^\\w\\s]+','',x))\n        return data\n    \n    def remove_stopwords(data = None, stopwords = None):\n        \"\"\"\n        Removes Stopwords\n        Returns : Pandas DataFrame\n        \"\"\"\n        data['text'] = data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stopwords))\n        return data\n    \n    def remove_url(match_expr = r'([^\\s]+www[^\\s]+)|([^\\s]+https?[^\\s]+)', data = None):\n        \"\"\"\n        Removes URL that contain http|https|www using Regular Expressions\n        Returns : Pandas DataFrame\n        \"\"\"\n        data['text'] = data['text'].apply(lambda x : re.sub(match_expr,' ',x))\n        return data\n    \n    def remove_long_words(max_len = 30, data = None):\n        \"\"\"\n        Removes blob of characters that are longer than 30 characters\n        Returns : Pandas DataFrame\n        \"\"\"\n        data['text'] = data['text'].apply(lambda x : re.sub(r'[^\\s]{30,}',' ',x))\n        return data\n\n    def remove_multiple_spaces(data = None):\n        \"\"\"\n        Removes continually occurin spaces.\n        Returns : Pandas DataFrame\n        \"\"\"\n        data['text'] = data['text'].apply(lambda x : re.sub(r'\\s{2,}',' ',x))\n        return data\n    \n    def get_tokens(data = None):\n        train_tokens = data['text'].apply(lambda x : word_tokenize(x))\n        return train_tokens\n    \n    def get_lemmatization(token_list = None):\n        lemmatizer = WordNetLemmatizer()\n        lemmatized_list = []\n        lemmatized_sent = []\n\n        for tokens in token_list:\n            lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n            lemmatized_sent.append(\" \".join(lemmatized_tokens))\n        \n        return lemmatized_sent\n        \n    data = lower_text(data=data)\n    data = remove_url(data=data)\n    data = remove_long_words(data=data)\n    if rm_stopwords:\n        data = remove_stopwords(data=data,stopwords=stopwords)\n    data = remove_punctuation(data)\n    data = remove_multiple_spaces(data=data)\n    data = get_tokens(data=data)\n    data = get_lemmatization(data)\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2022-09-26T12:00:59.791402Z","iopub.execute_input":"2022-09-26T12:00:59.791681Z","iopub.status.idle":"2022-09-26T12:00:59.804835Z","shell.execute_reply.started":"2022-09-26T12:00:59.791648Z","shell.execute_reply":"2022-09-26T12:00:59.804042Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"stopword_list = stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2022-09-26T12:01:00.163910Z","iopub.execute_input":"2022-09-26T12:01:00.164634Z","iopub.status.idle":"2022-09-26T12:01:00.170739Z","shell.execute_reply.started":"2022-09-26T12:01:00.164597Z","shell.execute_reply":"2022-09-26T12:01:00.170011Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"X = preprocess_data(data_train,stopword_list)\nX_valid = preprocess_data(data_valid,stopword_list)\nX_test = preprocess_data(data_test,stopword_list)","metadata":{"execution":{"iopub.status.busy":"2022-09-26T12:15:52.880048Z","iopub.execute_input":"2022-09-26T12:15:52.880332Z","iopub.status.idle":"2022-09-26T12:17:09.785993Z","shell.execute_reply.started":"2022-09-26T12:15:52.880300Z","shell.execute_reply":"2022-09-26T12:17:09.785221Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"Xd = X\nX_validd = X_valid\nX_testd = X_test","metadata":{"execution":{"iopub.status.busy":"2022-09-26T12:17:16.839927Z","iopub.execute_input":"2022-09-26T12:17:16.840285Z","iopub.status.idle":"2022-09-26T12:17:16.847246Z","shell.execute_reply.started":"2022-09-26T12:17:16.840249Z","shell.execute_reply":"2022-09-26T12:17:16.846401Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nX = Xd\nX_valid = X_validd\nX_test = X_testd\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"size_of_vocabulary = 10000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = size_of_vocabulary , split=' ')\ntokenizer.fit_on_texts(X)","metadata":{"execution":{"iopub.status.busy":"2022-09-26T12:17:21.328885Z","iopub.execute_input":"2022-09-26T12:17:21.329439Z","iopub.status.idle":"2022-09-26T12:17:26.057360Z","shell.execute_reply.started":"2022-09-26T12:17:21.329399Z","shell.execute_reply":"2022-09-26T12:17:26.056595Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"X = tokenizer.texts_to_sequences(X)\nX_valid = tokenizer.texts_to_sequences(X_valid)\nX_test = tokenizer.texts_to_sequences(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-26T12:17:26.058833Z","iopub.execute_input":"2022-09-26T12:17:26.059107Z","iopub.status.idle":"2022-09-26T12:17:30.066178Z","shell.execute_reply.started":"2022-09-26T12:17:26.059070Z","shell.execute_reply":"2022-09-26T12:17:30.065249Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"X = tf.keras.preprocessing.sequence.pad_sequences(\n    X,\n    dtype='int',\n    padding='post',\n)\nX_valid = tf.keras.preprocessing.sequence.pad_sequences(\n    X_valid,\n    maxlen = X.shape[1],\n    dtype='int',\n    padding='post',\n)\nX_test = tf.keras.preprocessing.sequence.pad_sequences(\n    X_test,\n    maxlen= X.shape[1],\n    dtype='int',\n    padding='post',\n)","metadata":{"execution":{"iopub.status.busy":"2022-09-26T12:17:42.022544Z","iopub.execute_input":"2022-09-26T12:17:42.022818Z","iopub.status.idle":"2022-09-26T12:17:42.648715Z","shell.execute_reply.started":"2022-09-26T12:17:42.022788Z","shell.execute_reply":"2022-09-26T12:17:42.647946Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"execution":{"iopub.status.busy":"2022-09-26T12:18:07.915857Z","iopub.execute_input":"2022-09-26T12:18:07.916375Z","iopub.status.idle":"2022-09-26T12:18:07.921745Z","shell.execute_reply.started":"2022-09-26T12:18:07.916334Z","shell.execute_reply":"2022-09-26T12:18:07.921028Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"array([[1919,  763, 8093, ...,    0,    0,    0],\n       [ 143,    2,  171, ...,    0,    0,    0],\n       [  19,   32,  684, ...,    0,    0,    0],\n       ...,\n       [ 214,  188,    2, ...,    0,    0,    0],\n       [  23,  188, 2476, ...,    0,    0,    0],\n       [  12,    3,    7, ...,    0,    0,    0]])"},"metadata":{}}]},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-26T12:18:08.531163Z","iopub.execute_input":"2022-09-26T12:18:08.531738Z","iopub.status.idle":"2022-09-26T12:18:08.537049Z","shell.execute_reply.started":"2022-09-26T12:18:08.531701Z","shell.execute_reply":"2022-09-26T12:18:08.536273Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"(40000, 1165)"},"metadata":{}}]},{"cell_type":"code","source":"y = np.array(data_train['label'],dtype='float64')\ny_valid = np.array(data_valid['label'],dtype='float64')\ny_test = np.array(data_test['label'],dtype='float64')","metadata":{"execution":{"iopub.status.busy":"2022-09-26T12:18:09.243717Z","iopub.execute_input":"2022-09-26T12:18:09.244332Z","iopub.status.idle":"2022-09-26T12:18:09.249428Z","shell.execute_reply.started":"2022-09-26T12:18:09.244284Z","shell.execute_reply":"2022-09-26T12:18:09.248653Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def get_bidirectional_rnn(input_dim = None,output_dim = None,input_shape = None):\n\n    BiDirectional_layers = 3\n\n    model = tf.keras.Sequential()\n    model.add(\n        tf.keras.layers.Embedding(\n          input_dim = input_dim,\n          output_dim = output_dim,\n          input_length = input_shape[1]\n        )\n    )\n    ## difference between spatial and normal dropout layer\n    ## model.add(tf.keras.layers.Dropout(0.2),)\n\n    ## stacking LSTM layers require return_sequence to be set to true\n\n    for i in range(BiDirectional_layers):\n      model.add(\n          tf.keras.layers.Bidirectional(\n            tf.keras.layers.SimpleRNN(            \n              units = 16,\n              ## relu dont\n              activation=\"tanh\",\n              dropout=0.2,\n              return_sequences=True,\n            ),\n            merge_mode = \"concat\"\n        )\n      )\n    \n    model.add(\n        tf.keras.layers.GlobalMaxPooling1D(),\n    )\n    model.add(\n        tf.keras.layers.Dense(64,activation='relu')\n    )\n    model.add(\n        tf.keras.layers.Dense(32,activation='relu')\n    )\n    model.add(\n        tf.keras.layers.Dense(1,activation='sigmoid')\n    )\n    \n    model.compile(\n        \n        ## gradient clipping\n        optimizer=tf.keras.optimizers.Adam(clipvalue=0.005),\n        loss=tf.keras.losses.BinaryCrossentropy(),\n        metrics=tf.keras.metrics.BinaryAccuracy(),\n\n    )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-09-26T12:19:51.092488Z","iopub.execute_input":"2022-09-26T12:19:51.092791Z","iopub.status.idle":"2022-09-26T12:19:51.102497Z","shell.execute_reply.started":"2022-09-26T12:19:51.092757Z","shell.execute_reply":"2022-09-26T12:19:51.101426Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"model_bidirectional_rnn = get_bidirectional_rnn(input_dim = size_of_vocabulary ,\n                                                output_dim = 128,\n                                                input_shape = X.shape)","metadata":{"execution":{"iopub.status.busy":"2022-09-26T12:19:52.807560Z","iopub.execute_input":"2022-09-26T12:19:52.808125Z","iopub.status.idle":"2022-09-26T12:19:53.165955Z","shell.execute_reply.started":"2022-09-26T12:19:52.808084Z","shell.execute_reply":"2022-09-26T12:19:53.165135Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"model_bidirectional_rnn.summary()","metadata":{"execution":{"iopub.status.busy":"2022-09-26T12:19:55.081369Z","iopub.execute_input":"2022-09-26T12:19:55.081641Z","iopub.status.idle":"2022-09-26T12:19:55.093548Z","shell.execute_reply.started":"2022-09-26T12:19:55.081613Z","shell.execute_reply":"2022-09-26T12:19:55.092420Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (None, 1165, 128)         1280000   \n_________________________________________________________________\nbidirectional_13 (Bidirectio (None, 1165, 32)          4640      \n_________________________________________________________________\nbidirectional_14 (Bidirectio (None, 1165, 32)          1568      \n_________________________________________________________________\nbidirectional_15 (Bidirectio (None, 1165, 32)          1568      \n_________________________________________________________________\nglobal_max_pooling1d_3 (Glob (None, 32)                0         \n_________________________________________________________________\ndense_9 (Dense)              (None, 64)                2112      \n_________________________________________________________________\ndense_10 (Dense)             (None, 32)                2080      \n_________________________________________________________________\ndense_11 (Dense)             (None, 1)                 33        \n=================================================================\nTotal params: 1,292,001\nTrainable params: 1,292,001\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"early_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    min_delta=0.1,\n    patience=3,\n    mode=\"min\"\n)\nhistory_bidir_rnn = model_bidirectional_rnn.fit(\n    x = X,\n    y = y,\n    batch_size = 256,\n    validation_split = 0.2, \n    epochs = 5\n)","metadata":{"execution":{"iopub.status.busy":"2022-09-26T12:19:56.534113Z","iopub.execute_input":"2022-09-26T12:19:56.534718Z","iopub.status.idle":"2022-09-26T13:14:25.575878Z","shell.execute_reply.started":"2022-09-26T12:19:56.534680Z","shell.execute_reply":"2022-09-26T13:14:25.575172Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Epoch 1/5\n125/125 [==============================] - 660s 5s/step - loss: 0.4847 - binary_accuracy: 0.7701 - val_loss: 0.3333 - val_binary_accuracy: 0.8608\nEpoch 2/5\n125/125 [==============================] - 649s 5s/step - loss: 0.2623 - binary_accuracy: 0.8970 - val_loss: 0.2986 - val_binary_accuracy: 0.8863\nEpoch 3/5\n125/125 [==============================] - 652s 5s/step - loss: 0.1874 - binary_accuracy: 0.9321 - val_loss: 0.3189 - val_binary_accuracy: 0.8851\nEpoch 4/5\n125/125 [==============================] - 653s 5s/step - loss: 0.1382 - binary_accuracy: 0.9524 - val_loss: 0.3603 - val_binary_accuracy: 0.8721\nEpoch 5/5\n125/125 [==============================] - 654s 5s/step - loss: 0.0976 - binary_accuracy: 0.9677 - val_loss: 0.4018 - val_binary_accuracy: 0.8746\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Observations\n1. Model Architecture  \n  * Using more than 3 Bidirectional layers almost always leads to a stagnant loss that causes accuracy to be not more than 50%.\n  * No amount of gradient clipping was able to fix this.\n  * Another possible solution is to use less units in RNN layer but the model does not learn many features on a large vocabulary size\n\n2. Overfit\n  * Model Overfits the training dataset.\n  * Even after use of dropouts, the model is unable to learn all the features of the dataset.\n  * A small amount of error can be attributed to the ambiguity of some comments, where it is difficult to assess the sentiment of the text, even by humans.\n  * Future scope may use experimenting with larger architecture with use of recurrent dropouts.\n3. Gradient Clipping\n  * Gradient Clipping is important while dealing with RNN.\n  * Using an Optimizer, clip gradients that are too big to avoid exploding and vanishing gradients","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}