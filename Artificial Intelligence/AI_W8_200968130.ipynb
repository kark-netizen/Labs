{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RmpGP5101HJM",
    "outputId": "da1c904e-0372-4f15-ede9-022f8488c305"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Surface(640x480x32 SW)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import utils\n",
    "from gym.envs.registration import register\n",
    "from gym.envs.toy_text.frozen_lake import LEFT, RIGHT, DOWN, UP\n",
    "import time\n",
    "import plotly.express as px\n",
    "from IPython import display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# to use the render function in colab\n",
    "import os\n",
    "os.environ['SDL_VIDEODRIVER']='dummy'\n",
    "import pygame\n",
    "pygame.display.set_mode((640,480))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v75hv3s_wL7c"
   },
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0rqugN4dcwNo"
   },
   "outputs": [],
   "source": [
    "# function to print human-readable policy\n",
    "def print_policy(policy, action_names):\n",
    "    str_policy = policy.astype('str')\n",
    "    for action_num, action_name in action_names.items():\n",
    "        np.place(str_policy, policy == action_num, action_name)\n",
    "\n",
    "    print(str_policy[0:4])\n",
    "    print(str_policy[4:8])\n",
    "    print(str_policy[8:12])\n",
    "    print(str_policy[12:16])\n",
    "    \n",
    "    return str_policy\n",
    "\n",
    "action_names = {LEFT: 'LEFT', RIGHT: 'RIGHT', DOWN: 'DOWN', UP: 'UP'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NZs1TnJwj7S"
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "h0Ht-kbHHL4p"
   },
   "outputs": [],
   "source": [
    "# registering the 4x4 deterministic Frozen Lake environment with the Gym library\n",
    "register(\n",
    "    id='Deterministic-4x4-FrozenLake-v0',\n",
    "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '4x4',\n",
    "            'is_slippery': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5G0H95F_h5Rd"
   },
   "outputs": [],
   "source": [
    "# creating the environment and setting 0.9 as the discount rate gamma\n",
    "env = gym.make('Deterministic-4x4-FrozenLake-v0')\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncRa_sPzw2qi"
   },
   "source": [
    "## Policy Iteration\n",
    "\n",
    "Starting with a single random policy, Policy Iteration then evaluates that policy to determine its value function. The action that maximises the value function for each state is then chosen to improve the policy. Until the policy converges on an ideal policy, this process continues. In a limited number of iterations, policy iteration ensures convergence to the best course of action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NY_Fe1pChOHq"
   },
   "outputs": [],
   "source": [
    "# function to evaluate the value function from a given policy\n",
    "def policy_evaluation(env, gamma, policy, value_func_old, max_iterations=int(1e3), theta=1e-3):\n",
    "    \n",
    "    value_func_new = np.zeros(env.nS)\n",
    "    value_func_collect = np.zeros((env.nS,max_iterations))\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        \n",
    "        delta=0\n",
    "        \n",
    "        for s in range(env.nS):\n",
    "            value_func_temp = 0\n",
    "            a = policy[s]\n",
    "            \n",
    "            # Using env.P[s][a] implement the V(s) updation given in Figure 4.1\n",
    "            # Note: Since this is a deterministic policy, the summation over\n",
    "            # action space is not required.\n",
    "            for next_state_from_tpm in env.P[s][a]:\n",
    "                prob_action = next_state_from_tpm[0]\n",
    "                cur_reward=next_state_from_tpm[2]\n",
    "                future_reward=gamma*value_func_old[next_state_from_tpm[1]]\n",
    "                value_func_temp+=prob_action*(cur_reward+future_reward)\n",
    "            \n",
    "            diff = abs(value_func_old[s]-value_func_temp)\n",
    "            delta = max(delta,diff)\n",
    "            \n",
    "            value_func_new[s] = value_func_temp\n",
    "        \n",
    "        # Stopping criteria: STOP when the sup norm of (V_k-V_{k-1}) is less than some theta\n",
    "        if delta <= theta: break\n",
    "        \n",
    "        value_func_old = value_func_new\n",
    "        value_func_collect[:,iteration] = value_func_old\n",
    "\n",
    "    return delta, value_func_new, iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ci7gCMQ_hd01"
   },
   "outputs": [],
   "source": [
    "# function to improve the policy given the value function\n",
    "def policy_improvement(env, gamma, value_func, policy):\n",
    "    \"\"\"\n",
    "      Given a policy and value function, improve the policy.\n",
    "      Returns true if policy is unchanged. Also returns the new policy.\n",
    "      See section 4.2 of Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning) by Sutton and Barto\n",
    "    \"\"\"\n",
    "    value_func_new = np.zeros(env.nS)\n",
    "    policy_stable=True\n",
    "    for s in range(env.nS):\n",
    "        old_action=policy[s]\n",
    "        max_value_func=-1\n",
    "        max_action=-1\n",
    "        for a in range(env.nA):\n",
    "            value_func_temp=0\n",
    "            # Copy and paste the \"sum\" which you've implemented in the \n",
    "            # policy_evaluation function here.\n",
    "            for next_state_from_tpm in env.P[s][a]:\n",
    "                prob_action = next_state_from_tpm[0]\n",
    "                cur_reward = next_state_from_tpm[2]\n",
    "                future_reward = gamma * value_func[next_state_from_tpm[1]]\n",
    "                value_func_temp+=prob_action * (cur_reward + future_reward)\n",
    "            \n",
    "            if value_func_temp>max_value_func:\n",
    "                max_value_func=value_func_temp\n",
    "                max_action=a\n",
    "        if max_action!=old_action: policy_stable=False\n",
    "        policy[s]=max_action\n",
    "        value_func_new[s]=max_value_func\n",
    "    return policy_stable, policy, value_func_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TVwFzunchhpK"
   },
   "outputs": [],
   "source": [
    "# function to implement iteration\n",
    "def policy_iteration(env, gamma, nS, nA, max_iterations=int(1e3), theta=1e-3):\n",
    "    \"\"\"\n",
    "       Runs policy iteration.\n",
    "       Returns optimal policy, value function, number of policy\n",
    "       improvement iterations, and number of value iterations.\n",
    "    \"\"\"\n",
    "    env.nS = nS\n",
    "    env.nA = nA\n",
    "    policy = np.random.randint(4, size = env.nS)\n",
    "    value_func_old = np.random.rand(env.nS)\n",
    "    value_func = np.zeros(env.nS)\n",
    "    value_func_collect = np.zeros((env.nS, max_iterations))\n",
    "    delta_collect = np.zeros(max_iterations)\n",
    "    policy_stable = False\n",
    "    iters = 0\n",
    "    eval_iters = 0\n",
    "    while not policy_stable:\n",
    "        delta, value_func, iter = policy_evaluation(env, gamma, policy, value_func_old)\n",
    "        delta_collect[iters] = delta\n",
    "        value_func_collect[:, iters] = value_func\n",
    "        eval_iters += iter\n",
    "        policy_stable, policy, value_func_old = policy_improvement(env, gamma, value_func, policy)\n",
    "        iters += 1\n",
    "\n",
    "    return policy, value_func, iters, eval_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89d2VF2Jhm5L",
    "outputId": "bcec2606-b418-47ef-dd67-ff70814d4234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Beginning Policy Iteration\n",
      "-------------------------\n",
      "\n",
      "Total time taken: 0.012122154235839844\n",
      "Total Policy Improvement Steps: 5\n",
      "Total Policy Evaluation Steps: 48\n",
      "\n",
      "Value function:\n",
      " [5.91324620e-01 6.56934620e-01 7.29834620e-01 6.56934620e-01\n",
      " 6.56934620e-01 2.20173279e-03 8.10834620e-01 1.31170468e-03\n",
      " 7.29834620e-01 8.10834620e-01 9.00834620e-01 2.44751255e-03\n",
      " 1.08965375e-03 9.00834620e-01 1.00083462e+00 8.34620454e-04]\n",
      "\n",
      "Policy:\n",
      "['DOWN' 'RIGHT' 'DOWN' 'LEFT']\n",
      "['DOWN' 'LEFT' 'DOWN' 'LEFT']\n",
      "['RIGHT' 'DOWN' 'DOWN' 'LEFT']\n",
      "['LEFT' 'RIGHT' 'RIGHT' 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "# displaying performance of policy iteration in an episode\n",
    "print(\"-\"*25 + \"\\nBeginning Policy Iteration\\n\" + \"-\"*25)\n",
    "start_time = time.time()\n",
    "policy, value_func, policy_iters, val_iters = policy_iteration(env, gamma, 16, 4)\n",
    "\n",
    "print(\"\\nTotal time taken: \" + str((time.time() - start_time)))\n",
    "print(\"Total Policy Improvement Steps: \" + str(policy_iters))\n",
    "print(\"Total Policy Evaluation Steps: \" + str(val_iters))\n",
    "\n",
    "print(\"\\nValue function:\\n\", value_func)\n",
    "\n",
    "print(\"\\nPolicy:\")\n",
    "policy_str = print_policy(policy, action_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqSDhIZN0FL-"
   },
   "source": [
    "## Value Iteration\n",
    "\n",
    "Value Iteration updates a random initial value function iteratively until it finds the best value function. The action that maximises the sum of the immediate reward and the discounted value of the following state is chosen for each iteration in order to update the value of each state. Until the value function converges to the ideal value function, the process is repeated. Moreover, value iteration ensures that the ideal value function will be reached in a limited amount of repetitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4Tamf9VIk7M7"
   },
   "outputs": [],
   "source": [
    "# function to implement iteration\n",
    "def value_iteration(env, gamma, max_iterations=int(1e3), theta=1e-3):\n",
    "    \"\"\"\n",
    "    Runs value iteration for a given gamma and environment. Return \n",
    "    the value function and the number of iterations it took to converge.\n",
    "    \"\"\"\n",
    "    value_func_old = np.random.rand(env.nS)\n",
    "    value_func_new = np.zeros(env.nS)\n",
    "    value_func_collect = np.zeros((env.nS,max_iterations))\n",
    "    delta_collect = np.zeros(max_iterations)\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        delta=0\n",
    "        for s in range(env.nS):\n",
    "            max_value_func = -1\n",
    "            \n",
    "            # Refer Figure 4.5 for the algorithm\n",
    "            # Parse through env.nA for finding the maximum\n",
    "            # Using env.P[s][a] implement the V(s) updation\n",
    "            for a in range(env.nA):\n",
    "                value_func_temp = 0\n",
    "                for next_state_from_tpm in env.P[s][a]: \n",
    "                    prob_action = next_state_from_tpm[0]\n",
    "                    cur_reward=next_state_from_tpm[2]\n",
    "                    if next_state_from_tpm[3]:\n",
    "                        future_reward=0\n",
    "                    else: future_reward = gamma * value_func_old[next_state_from_tpm[1]]\n",
    "                    value_func_temp += prob_action * (cur_reward + future_reward)\n",
    "                if value_func_temp > max_value_func:\n",
    "                    max_value_func = value_func_temp\n",
    "                    \n",
    "            diff = abs(value_func_old[s] - max_value_func)\n",
    "            delta = max(delta, diff)\n",
    "            value_func_new[s] = max_value_func\n",
    "\n",
    "        delta_collect[iteration] = delta\n",
    "        value_func_old = value_func_new\n",
    "        value_func_collect[:, iteration] = value_func_old\n",
    "        # Stopping criteria: STOP when the sup norm of (V_k-V_{k-1}) is less than some theta\n",
    "        if delta <= theta: break\n",
    "    return value_func_new, iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_uYnm7OkmYQJ"
   },
   "outputs": [],
   "source": [
    "# function to convert value function to policy\n",
    "def value_function_to_policy(env, gamma, value_function):\n",
    "    \"\"\"\n",
    "    Mapping actions for each state using the value_function to get a policy\n",
    "    \"\"\"\n",
    "    policy = np.zeros(env.nS, dtype='int')\n",
    "    for s in range(env.nS):\n",
    "        max_value_func = -1\n",
    "        max_action = -1\n",
    "        for a in range(env.nA):\n",
    "            value_func_temp = 0\n",
    "            for next_state_from_tpm in env.P[s][a]:\n",
    "                prob_action = next_state_from_tpm[0]\n",
    "                cur_reward = next_state_from_tpm[2]\n",
    "                future_reward = gamma * value_function[next_state_from_tpm[1]]\n",
    "                value_func_temp += prob_action * (cur_reward + future_reward)\n",
    "            if value_func_temp > max_value_func:\n",
    "                max_value_func = value_func_temp\n",
    "                max_action = a\n",
    "        policy[s] = max_action\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1USP3AAUmZBG",
    "outputId": "60f958c2-6fb5-4381-94a2-841126dd34c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Beginning Value Iteration\n",
      "-------------------------\n",
      "\n",
      "Total time taken: 0.0021886825561523438\n",
      "Total Value Iteration Steps: 6\n",
      "\n",
      "Value function:\n",
      " [0.59049 0.6561  0.729   0.6561  0.6561  0.      0.81    0.      0.729\n",
      " 0.81    0.9     0.      0.      0.9     1.      0.     ]\n",
      "\n",
      "Policy:\n",
      "['DOWN' 'RIGHT' 'DOWN' 'LEFT']\n",
      "['DOWN' 'LEFT' 'DOWN' 'LEFT']\n",
      "['RIGHT' 'DOWN' 'DOWN' 'LEFT']\n",
      "['LEFT' 'RIGHT' 'RIGHT' 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "# displaying performance of value iteration in an episode\n",
    "print(\"-\"*25 + \"\\nBeginning Value Iteration\\n\" + \"-\"*25)\n",
    "start_time = time.time()\n",
    "value_function, value_iters=value_iteration(env, gamma)\n",
    "\n",
    "print(\"\\nTotal time taken: \" + str((time.time() - start_time)))\n",
    "print(\"Total Value Iteration Steps: \" + str(value_iters))\n",
    "print(\"\\nValue function:\\n \" + str(value_function))\n",
    "\n",
    "print(\"\\nPolicy:\")\n",
    "policy = value_function_to_policy(env, gamma, value_function)\n",
    "policy_str = print_policy(policy, action_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ompMGzL41V83"
   },
   "source": [
    "## 1000 Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sn63IywGhq1w",
    "outputId": "877db962-fb3f-4813-e654-3e5c4c5335fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wins:  1000\n",
      "Average return:  1.0\n"
     ]
    }
   ],
   "source": [
    "# running policy iteration for 1000 episodes\n",
    "num_episodes = 1000\n",
    "wins = 0\n",
    "returns = []\n",
    "policy, value_func, policy_iters, val_iters = policy_iteration(env, gamma, 16, 4)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    episode_return = 0\n",
    "    while not done:\n",
    "        action = policy[obs]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_return += reward\n",
    "    returns.append(episode_return)\n",
    "    if episode_return > 0:\n",
    "        wins += 1\n",
    "\n",
    "avg_return = sum(returns) / num_episodes\n",
    "print(\"Number of wins: \", wins)\n",
    "print(\"Average return: \", avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AA3uuS1hmt0B",
    "outputId": "b778f6c3-168c-4dd7-e580-e35fde894d77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wins:  1000\n",
      "Average return:  1.0\n"
     ]
    }
   ],
   "source": [
    "# running value iteration for 1000 episodes\n",
    "wins = 0\n",
    "returns = []\n",
    "value_function, value_iters=value_iteration(env, gamma)\n",
    "policy = value_function_to_policy(env, gamma, value_function)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    episode_return = 0\n",
    "    while not done:\n",
    "        action = policy[obs]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_return += reward\n",
    "    returns.append(episode_return)\n",
    "    if episode_return > 0:\n",
    "        wins += 1\n",
    "\n",
    "avg_return = sum(returns) / num_episodes\n",
    "print(\"Number of wins: \", wins)\n",
    "print(\"Average return: \", avg_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZJbmwbH1boF"
   },
   "source": [
    "## Episode Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MEcf9TQi33V6"
   },
   "outputs": [],
   "source": [
    "# function to render one episode with the obtained optimal policy\n",
    "def render_single(env, policy, max_steps=100):\n",
    "  \"\"\"\n",
    "    This function does not need to be modified\n",
    "    Renders policy once on environment. Watch your agent play!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "  \"\"\"\n",
    "\n",
    "  episode_reward = 0\n",
    "  obs = env.reset()\n",
    "  for i in range(max_steps):\n",
    "    action = policy[obs]\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    episode_reward += reward\n",
    "    img = env.render(mode='rgb_array')\n",
    "    display.clear_output(wait=True)\n",
    "    fig = px.imshow(img)\n",
    "    fig.update_layout(autosize=False, width=500, height=500, margin=dict(l=10, r=10, t=10, b=10))\n",
    "    fig.show()\n",
    "    if done:\n",
    "        break\n",
    "  if not done:\n",
    "    print(\"\\nThe agent didn't reach a terminal state in {} steps.\\n\".format(max_steps))\n",
    "  else:\n",
    "  \tprint(\"\\nEpisode reward: %f\\n\" % episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "id": "fo47nkRmnbq8",
    "outputId": "b3369289-c780-4e0f-db3a-56c866add1ba"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"1de439d8-2b72-4688-9e25-8a3f999b3a7b\" class=\"plotly-graph-div\" style=\"height:500px; width:500px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1de439d8-2b72-4688-9e25-8a3f999b3a7b\")) {                    Plotly.newPlot(                        \"1de439d8-2b72-4688-9e25-8a3f999b3a7b\",                        [{\"name\":\"0\",\"source\":\"data:image/png;base64,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\",\"type\":\"image\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: [%{z[0]}, %{z[1]}, %{z[2]}]<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"margin\":{\"t\":10,\"l\":10,\"r\":10,\"b\":10},\"autosize\":false,\"width\":500,\"height\":500},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('1de439d8-2b72-4688-9e25-8a3f999b3a7b');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode reward: 1.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "render_single(env, policy, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obNFy4fR1lVA"
   },
   "source": [
    "The agent was able to reach the goal state without falling into a hole using the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMNZ_jQq1wDW"
   },
   "source": [
    "## Overall Inference\n",
    "\n",
    "Both algorithms found the optimal policy and won every episode.\n",
    "\n",
    "The main difference between policy iteration and value iteration is that policy iteration requires policy evaluation at each step, while value iteration performs policy evaluation implicitly by iteratively updating the value function. This makes value iteration more computationally efficient than policy iteration. \n",
    "\n",
    "However, since the Frozen Lake environment is a relatively simple environment with a small state space, it is easier for both policy iteration and value iteration to converge to optimal solutions quickly."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
