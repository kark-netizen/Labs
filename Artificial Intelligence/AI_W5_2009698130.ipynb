{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vy23D_WMhRmS"
   },
   "source": [
    "# WEEK 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pL2ZxoTYhTSI"
   },
   "source": [
    "1.Do the tutorial for MAB in TF-Agents https://www.tensorflow.org/agents/tutorials/bandits_tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9v1tZ3thkYR"
   },
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xn2w4D6xhgNJ"
   },
   "source": [
    "### Create a environment -:\n",
    "\n",
    "#### a. for  which  the  observation  is  a  random  integer  between -5 and 5,  there  are  3 possible actions (0, 1, 2), and the reward is the product of the action and the observation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "M9QAAY8Al-Px",
    "outputId": "76d085ef-8162-4fd3-bc7a-49b24c02aaa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting tf-agents\n",
      "  Downloading tf_agents-0.15.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (4.5.0)\n",
      "Collecting pygame==2.1.0\n",
      "  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (3.19.6)\n",
      "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.5.0)\n",
      "Collecting gym<=0.23.0,>=0.17.0\n",
      "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 KB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from tf-agents) (8.4.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.22.4)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.4.0)\n",
      "Requirement already satisfied: tensorflow-probability>=0.18.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.19.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.15.0)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (6.0.0)\n",
      "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.4.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (4.4.2)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.1.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents) (3.15.0)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697658 sha256=540fe6d8f93f18116c9f24e4f6a48fe1bf614e0d1bf10d92f6bf2ac523b00ef0\n",
      "  Stored in directory: /root/.cache/pip/wheels/e7/2f/ab/68bf956c5dde73c1856d981e54292cf58385fb60bca10b7acd\n",
      "Successfully built gym\n",
      "Installing collected packages: pygame, gym, tf-agents\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.25.2\n",
      "    Uninstalling gym-0.25.2:\n",
      "      Successfully uninstalled gym-0.25.2\n",
      "Successfully installed gym-0.23.0 pygame-2.1.0 tf-agents-0.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "G5_j3XJBl4pE"
   },
   "outputs": [],
   "source": [
    "import abc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents import tf_agent\n",
    "from tf_agents.drivers import driver\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.policies import tf_policy\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import policy_step\n",
    "\n",
    "nest = tf.nest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RXC4Gig-mmyB"
   },
   "outputs": [],
   "source": [
    "class BanditPyEnvironment(py_environment.PyEnvironment):\n",
    "\n",
    "  def __init__(self, observation_spec, action_spec):\n",
    "    self._observation_spec = observation_spec\n",
    "    self._action_spec = action_spec\n",
    "    super(BanditPyEnvironment, self).__init__()\n",
    "\n",
    "  # Helper functions.\n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return self._observation_spec\n",
    "\n",
    "  def _empty_observation(self):\n",
    "    return tf.nest.map_structure(lambda x: np.zeros(x.shape, x.dtype),\n",
    "                                 self.observation_spec())\n",
    "\n",
    "  # These two functions below should not be overridden by subclasses.\n",
    "  def _reset(self):\n",
    "    \"\"\"Returns a time step containing an observation.\"\"\"\n",
    "    return ts.restart(self._observe(), batch_size=self.batch_size)\n",
    "\n",
    "  def _step(self, action):\n",
    "    \"\"\"Returns a time step containing the reward for the action taken.\"\"\"\n",
    "    reward = self._apply_action(action)\n",
    "    return ts.termination(self._observe(), reward)\n",
    "\n",
    "  # These two functions below are to be implemented in subclasses.\n",
    "  @abc.abstractmethod\n",
    "  def _observe(self):\n",
    "    \"\"\"Returns an observation.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def _apply_action(self, action):\n",
    "    \"\"\"Applies `action` to the Environment and returns the corresponding reward.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EV1JftvCpi6T"
   },
   "outputs": [],
   "source": [
    "class BanditEnv(BanditPyEnvironment):\n",
    "\n",
    "  def __init__(self):\n",
    "    action_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
    "    observation_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(1,), dtype=np.int32, minimum=-5, maximum=5, name='observation')\n",
    "    super(BanditEnv, self).__init__(observation_spec, action_spec)\n",
    "\n",
    "  def _observe(self):\n",
    "    self._observation = np.random.randint(-5, 6, (1,), dtype='int32')\n",
    "    return self._observation\n",
    "\n",
    "  def _apply_action(self, action):\n",
    "    if self._observation < 0:\n",
    "      return 0\n",
    "    else:\n",
    "      return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wlK7alvCoLLQ"
   },
   "outputs": [],
   "source": [
    "env = BanditEnv()\n",
    "tf_environment = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i058nvmgoZzx"
   },
   "source": [
    "#### b. Define an optimal policy manually. The action only depends on the sign of the observation, 0 when is negative and 2 when is positive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ytC11ILfmaHZ"
   },
   "outputs": [],
   "source": [
    "class SignPolicy(tf_policy.TFPolicy):\n",
    "  def __init__(self):\n",
    "    observation_spec = tensor_spec.BoundedTensorSpec(\n",
    "        shape=(1,), dtype=tf.int32, minimum=-2, maximum=2)\n",
    "    time_step_spec = ts.time_step_spec(observation_spec)\n",
    "\n",
    "    action_spec = tensor_spec.BoundedTensorSpec(\n",
    "        shape=(), dtype=tf.int32, minimum=0, maximum=2)\n",
    "\n",
    "    super(SignPolicy, self).__init__(time_step_spec=time_step_spec,\n",
    "                                     action_spec=action_spec)\n",
    "  def _distribution(self, time_step):\n",
    "    pass\n",
    "\n",
    "  def _variables(self):\n",
    "    return ()\n",
    "\n",
    "  def _action(self, time_step, policy_state, seed):\n",
    "    observation_sign = tf.cast(tf.sign(time_step.observation[0]), dtype=tf.int32)\n",
    "    action = observation_sign + 1\n",
    "    return policy_step.PolicyStep(action, policy_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rH9IaUKaob9h"
   },
   "source": [
    "#### c. Request  for 50 observations  from  the  environment, compute  and  print the total reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wleOFEQouDR"
   },
   "source": [
    "We request an observation from the environment, call the policy to choose an action, then the environment will output the reward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H002NHFAmwso",
    "outputId": "89b05c74-52d8-4aae-d75a-5ad8a4f99506"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation:\n",
      "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[5]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-5]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-4]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-3]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-4]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[2]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-4]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-5]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[2]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-4]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[2]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[5]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-5]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-3]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-5]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[2]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[3]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-4]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[4]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[4]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[5]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[5]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[3]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-3]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-4]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[4]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-5]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[2]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[2]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-3]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[2]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-4]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[4]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "Observation:\n",
      "tf.Tensor([[-5]], shape=(1, 1), dtype=int32)\n",
      "Action:\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sign_policy = SignPolicy()\n",
    "for i in range(50):\n",
    "  current_time_step = tf_environment.reset()\n",
    "  print('Observation:')\n",
    "  print (current_time_step.observation)\n",
    "  action = sign_policy.action(current_time_step).action\n",
    "  print('Action:')\n",
    "  print (action)\n",
    "  reward = tf_environment.step(action).reward\n",
    "  print('Reward:')\n",
    "  print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onkKPAtPpE9_"
   },
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_qpe5OFpIVi"
   },
   "source": [
    "Create an environment \n",
    "\n",
    "a.Define an environment will either always give reward = observation * action or reward = -observation * action. This will be decided when the environment is initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61wD7NR_o7Fy",
    "outputId": "43b051b4-acbf-45a2-8e7b-4673701644cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward sign:\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "class TwoWayPyEnvironment(BanditPyEnvironment):\n",
    "\n",
    "  def __init__(self):\n",
    "    action_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
    "    observation_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(1,), dtype=np.int32, minimum=-2, maximum=2, name='observation')\n",
    "\n",
    "    # Flipping the sign with probability 1/2.\n",
    "    self._reward_sign = 2 * np.random.randint(2) - 1\n",
    "    print(\"reward sign:\")\n",
    "    print(self._reward_sign)\n",
    "\n",
    "    super(TwoWayPyEnvironment, self).__init__(observation_spec, action_spec)\n",
    "\n",
    "  def _observe(self):\n",
    "    self._observation = np.random.randint(-2, 3, (1,), dtype='int32')\n",
    "    return self._observation\n",
    "\n",
    "  def _apply_action(self, action):\n",
    "    return self._reward_sign * action * self._observation[0]\n",
    "\n",
    "two_way_tf_environment = tf_py_environment.TFPyEnvironment(TwoWayPyEnvironment())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubOSLnaypXWq"
   },
   "source": [
    "b.Define a policy that detects the behavior of the underlying environment. There are three situations that the policy needs to handle:\n",
    "\n",
    "  - The agent has not detected know yet which version of the environment is running.\n",
    "\n",
    "  - The  agent  detected  that  the  original  version  of  the  environment  is running.\n",
    "\n",
    "  - The  agent  detected  that  the  flipped  version  of  the  environment  is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0Ste4_KBo7Dt"
   },
   "outputs": [],
   "source": [
    "class TwoWaySignPolicy(tf_policy.TFPolicy):\n",
    "  def __init__(self, situation):\n",
    "    observation_spec = tensor_spec.BoundedTensorSpec(\n",
    "        shape=(1,), dtype=tf.int32, minimum=-2, maximum=2)\n",
    "    action_spec = tensor_spec.BoundedTensorSpec(\n",
    "        shape=(), dtype=tf.int32, minimum=0, maximum=2)\n",
    "    time_step_spec = ts.time_step_spec(observation_spec)\n",
    "    self._situation = situation\n",
    "    super(TwoWaySignPolicy, self).__init__(time_step_spec=time_step_spec,\n",
    "                                           action_spec=action_spec)\n",
    "  def _distribution(self, time_step):\n",
    "    pass\n",
    "\n",
    "  def _variables(self):\n",
    "    return [self._situation]\n",
    "\n",
    "  def _action(self, time_step, policy_state, seed):\n",
    "    sign = tf.cast(tf.sign(time_step.observation[0, 0]), dtype=tf.int32)\n",
    "    def case_unknown_fn():\n",
    "      # Choose 1 so that we get information on the sign.\n",
    "      return tf.constant(1, shape=(1,))\n",
    "\n",
    "    # Choose 0 or 2, depending on the situation and the sign of the observation.\n",
    "    def case_normal_fn():\n",
    "      return tf.constant(sign + 1, shape=(1,))\n",
    "    def case_flipped_fn():\n",
    "      return tf.constant(1 - sign, shape=(1,))\n",
    "\n",
    "    cases = [(tf.equal(self._situation, 0), case_unknown_fn),\n",
    "             (tf.equal(self._situation, 1), case_normal_fn),\n",
    "             (tf.equal(self._situation, 2), case_flipped_fn)]\n",
    "    action = tf.case(cases, exclusive=True)\n",
    "    return policy_step.PolicyStep(action, policy_state)\n",
    "\n",
    "# two_way_policy = TwoWaySignPolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gtc5YV77psXo"
   },
   "source": [
    "c.Define the agent that detects the sign of the environment and sets the policy appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "DGBzcyDmo7BC"
   },
   "outputs": [],
   "source": [
    "class SignAgent(tf_agent.TFAgent):\n",
    "  def __init__(self):\n",
    "    self._situation = tf.Variable(0, dtype=tf.int32)\n",
    "    policy = TwoWaySignPolicy(self._situation)\n",
    "    time_step_spec = policy.time_step_spec\n",
    "    action_spec = policy.action_spec\n",
    "    super(SignAgent, self).__init__(time_step_spec=time_step_spec,\n",
    "                                    action_spec=action_spec,\n",
    "                                    policy=policy,\n",
    "                                    collect_policy=policy,\n",
    "                                    train_sequence_length=None)\n",
    "\n",
    "  def _initialize(self):\n",
    "    return tf.compat.v1.variables_initializer(self.variables)\n",
    "\n",
    "  def _train(self, experience, weights=None):\n",
    "    observation = experience.observation\n",
    "    action = experience.action\n",
    "    reward = experience.reward\n",
    "\n",
    "    # We only need to change the value of the situation variable if it is\n",
    "    # unknown (0) right now, and we can infer the situation only if the\n",
    "    # observation is not 0.\n",
    "    needs_action = tf.logical_and(tf.equal(self._situation, 0),\n",
    "                                  tf.not_equal(reward, 0))\n",
    "\n",
    "\n",
    "    def new_situation_fn():\n",
    "      \"\"\"This returns either 1 or 2, depending on the signs.\"\"\"\n",
    "      return (3 - tf.sign(tf.cast(observation[0, 0, 0], dtype=tf.int32) *\n",
    "                          tf.cast(action[0, 0], dtype=tf.int32) *\n",
    "                          tf.cast(reward[0, 0], dtype=tf.int32))) / 2\n",
    "\n",
    "    new_situation = tf.cond(needs_action,\n",
    "                            new_situation_fn,\n",
    "                            lambda: self._situation)\n",
    "    new_situation = tf.cast(new_situation, tf.int32)\n",
    "    tf.compat.v1.assign(self._situation, new_situation)\n",
    "    return tf_agent.LossInfo((), ())\n",
    "\n",
    "sign_agent = SignAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1_TmCJx6o6-i"
   },
   "outputs": [],
   "source": [
    "def trajectory_for_bandit(initial_step, action_step, final_step):\n",
    "  return trajectory.Trajectory(observation=tf.expand_dims(initial_step.observation, 0),\n",
    "                               action=tf.expand_dims(action_step.action, 0),\n",
    "                               policy_info=action_step.info,\n",
    "                               reward=tf.expand_dims(final_step.reward, 0),\n",
    "                               discount=tf.expand_dims(final_step.discount, 0),\n",
    "                               step_type=tf.expand_dims(initial_step.step_type, 0),\n",
    "                               next_step_type=tf.expand_dims(final_step.step_type, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnvqL6-csRGw"
   },
   "source": [
    "Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EetdIhsro67v",
    "outputId": "27a978cb-e09c-48d4-f5a7-fd63c64d214b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory(\n",
      "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>,\n",
      " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n",
      " 'policy_info': (),\n",
      " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-1.]], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>})\n",
      "Trajectory(\n",
      "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n",
      " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-2]]], dtype=int32)>,\n",
      " 'policy_info': (),\n",
      " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
      "Trajectory(\n",
      "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n",
      " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-2]]], dtype=int32)>,\n",
      " 'policy_info': (),\n",
      " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
      "Trajectory(\n",
      "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n",
      " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n",
      " 'policy_info': (),\n",
      " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
      "Trajectory(\n",
      "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n",
      " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n",
      " 'policy_info': (),\n",
      " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
      "Trajectory(\n",
      "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n",
      " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-2]]], dtype=int32)>,\n",
      " 'policy_info': (),\n",
      " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
      "Trajectory(\n",
      "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n",
      " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n",
      " 'policy_info': (),\n",
      " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
      "Trajectory(\n",
      "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n",
      " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-2]]], dtype=int32)>,\n",
      " 'policy_info': (),\n",
      " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
      "Trajectory(\n",
      "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n",
      " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n",
      " 'policy_info': (),\n",
      " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
      "Trajectory(\n",
      "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>,\n",
      " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[0]]], dtype=int32)>,\n",
      " 'policy_info': (),\n",
      " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n"
     ]
    }
   ],
   "source": [
    "# step = two_way_tf_environment.reset()\n",
    "for _ in range(10):\n",
    "  action_step = sign_agent.collect_policy.action(step)\n",
    "  next_step = two_way_tf_environment.step(action_step.action)\n",
    "  experience = trajectory_for_bandit(step, action_step, next_step)\n",
    "  print(experience)\n",
    "  sign_agent.train(experience)\n",
    "  step = next_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbgZu2YjsU9J"
   },
   "source": [
    "# The End "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
